"""
Utility functions for working with comment embeddings.

This module provides helper functions for loading, searching, and analyzing
comment embeddings generated by the generate_embeddings.py script.
"""

import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np


class EmbeddingStore:
    """Convenient interface for working with stored embeddings."""
    
    def __init__(self, embeddings_dir: str = "data/embeddings"):
        """
        Initialize the embedding store.
        
        Args:
            embeddings_dir: Directory containing embedding files
        """
        self.embeddings_dir = Path(embeddings_dir)
        self.embeddings_file = self.embeddings_dir / "comment_embeddings.json"
        self.index_file = self.embeddings_dir / "embedding_index.json"
        
        self._embeddings_cache = None
        self._metadata_cache = None
    
    def load_embeddings(self, reload: bool = False) -> Dict[str, Any]:
        """
        Load all embeddings from disk.
        
        Args:
            reload: Force reload from disk even if cached
            
        Returns:
            Dictionary with embeddings and metadata
        """
        if self._embeddings_cache is not None and not reload:
            return self._embeddings_cache
        
        if not self.embeddings_file.exists():
            raise FileNotFoundError(
                f"Embeddings file not found: {self.embeddings_file}\n"
                f"Run 'uv run python generate_embeddings.py' first to generate embeddings."
            )
        
        with open(self.embeddings_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self._embeddings_cache = data
        return data
    
    def load_index(self, reload: bool = False) -> Dict[str, Any]:
        """
        Load embedding index (metadata only, faster for browsing).
        
        Args:
            reload: Force reload from disk even if cached
            
        Returns:
            Dictionary with embedding metadata
        """
        if self._metadata_cache is not None and not reload:
            return self._metadata_cache
        
        if not self.index_file.exists():
            # Fall back to full embeddings file
            data = self.load_embeddings(reload)
            return data.get('embeddings', {})
        
        with open(self.index_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self._metadata_cache = data
        return data
    
    def get_embedding(self, case_id: str, evaluator: str) -> Optional[List[float]]:
        """
        Get embedding for a specific case and evaluator.
        
        Args:
            case_id: Case ID
            evaluator: Evaluator username
            
        Returns:
            Embedding vector or None if not found
        """
        data = self.load_embeddings()
        key = f"{case_id}_{evaluator}"
        
        embedding_data = data.get('embeddings', {}).get(key)
        return embedding_data['embedding'] if embedding_data else None
    
    def get_all_embeddings_matrix(self) -> Tuple[np.ndarray, List[str]]:
        """
        Get all embeddings as a numpy matrix.
        
        Returns:
            Tuple of (embeddings_matrix, list of keys)
        """
        data = self.load_embeddings()
        embeddings_dict = data.get('embeddings', {})
        
        keys = list(embeddings_dict.keys())
        embeddings = [embeddings_dict[k]['embedding'] for k in keys]
        
        return np.array(embeddings), keys
    
    def cosine_similarity(
        self,
        embedding1: List[float],
        embedding2: List[float]
    ) -> float:
        """
        Calculate cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding vector
            embedding2: Second embedding vector
            
        Returns:
            Cosine similarity score (0-1), or 0.0 if either vector is zero
        """
        e1 = np.array(embedding1)
        e2 = np.array(embedding2)
        
        dot_product = np.dot(e1, e2)
        norm1 = np.linalg.norm(e1)
        norm2 = np.linalg.norm(e2)
        
        # Prevent division by zero for zero vectors
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def find_similar(
        self,
        query_embedding: List[float],
        top_k: int = 10,
        exclude_keys: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        Find most similar embeddings to a query embedding.
        
        Args:
            query_embedding: Query embedding vector
            top_k: Number of results to return
            exclude_keys: Keys to exclude from search (e.g., the query itself)
            
        Returns:
            List of dictionaries with similarity scores and metadata
        """
        data = self.load_embeddings()
        embeddings_dict = data.get('embeddings', {})
        
        exclude_keys = exclude_keys or []
        similarities = []
        
        for key, val in embeddings_dict.items():
            if key in exclude_keys:
                continue
            
            embedding = val['embedding']
            similarity = self.cosine_similarity(query_embedding, embedding)
            
            similarities.append({
                'key': key,
                'case_id': val['case_id'],
                'evaluator': val['evaluator'],
                'comments': val['comments'],
                'decision': val['decision'],
                'problem_axes': val['problem_axes'],
                'similarity': float(similarity)
            })
        
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]
    
    def find_similar_to_comment(
        self,
        case_id: str,
        evaluator: str,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Find comments similar to a specific evaluation.
        
        Args:
            case_id: Case ID
            evaluator: Evaluator username
            top_k: Number of results to return
            
        Returns:
            List of similar comments with scores
        """
        query_embedding = self.get_embedding(case_id, evaluator)
        
        if query_embedding is None:
            raise ValueError(f"No embedding found for case {case_id} by {evaluator}")
        
        exclude_key = f"{case_id}_{evaluator}"
        return self.find_similar(query_embedding, top_k, exclude_keys=[exclude_key])
    
    def cluster_comments(
        self,
        n_clusters: int = 5,
        method: str = 'kmeans'
    ) -> Dict[str, Any]:
        """
        Cluster comments based on embeddings.
        
        Args:
            n_clusters: Number of clusters
            method: Clustering method ('kmeans' or 'hierarchical')
            
        Returns:
            Dictionary with cluster assignments and statistics
        """
        try:
            from sklearn.cluster import KMeans, AgglomerativeClustering
        except ImportError:
            raise ImportError(
                "scikit-learn is required for clustering. "
                "Install with: uv add --group analysis scikit-learn"
            )
        
        embeddings_matrix, keys = self.get_all_embeddings_matrix()
        
        if method == 'kmeans':
            clusterer = KMeans(n_clusters=n_clusters, random_state=42)
        elif method == 'hierarchical':
            clusterer = AgglomerativeClustering(n_clusters=n_clusters)
        else:
            raise ValueError(f"Unknown clustering method: {method}")
        
        labels = clusterer.fit_predict(embeddings_matrix)
        
        # Organize results by cluster
        clusters = {}
        data = self.load_embeddings()
        embeddings_dict = data.get('embeddings', {})
        
        for key, label in zip(keys, labels):
            label = int(label)
            if label not in clusters:
                clusters[label] = []
            
            clusters[label].append({
                'key': key,
                'case_id': embeddings_dict[key]['case_id'],
                'evaluator': embeddings_dict[key]['evaluator'],
                'comments': embeddings_dict[key]['comments'],
                'decision': embeddings_dict[key]['decision'],
                'problem_axes': embeddings_dict[key]['problem_axes']
            })
        
        return {
            'n_clusters': n_clusters,
            'method': method,
            'clusters': clusters,
            'cluster_sizes': {label: len(items) for label, items in clusters.items()}
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about stored embeddings.
        
        Returns:
            Dictionary with statistics
        """
        data = self.load_embeddings()
        metadata = data.get('metadata', {})
        embeddings_dict = data.get('embeddings', {})
        
        # Count by evaluator
        evaluator_counts = {}
        decision_counts = {'approve': 0, 'reject': 0}
        problem_axes_counts = {}
        
        for val in embeddings_dict.values():
            evaluator = val['evaluator']
            evaluator_counts[evaluator] = evaluator_counts.get(evaluator, 0) + 1
            
            decision = val['decision']
            decision_counts[decision] = decision_counts.get(decision, 0) + 1
            
            if val.get('problem_axes'):
                for axis in val['problem_axes']:
                    problem_axes_counts[axis] = problem_axes_counts.get(axis, 0) + 1
        
        return {
            'total_embeddings': metadata.get('total_embeddings', len(embeddings_dict)),
            'embedding_dimension': metadata.get('embedding_dimension', 0),
            'model': metadata.get('model', 'unknown'),
            'generated_at': metadata.get('generated_at', 'unknown'),
            'evaluator_counts': evaluator_counts,
            'decision_counts': decision_counts,
            'problem_axes_counts': problem_axes_counts
        }


def main():
    """CLI for exploring embeddings."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Utility for exploring and analyzing comment embeddings"
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Command to run')
    
    # Stats command
    subparsers.add_parser('stats', help='Show embedding statistics')
    
    # Similar command
    similar_parser = subparsers.add_parser('similar', help='Find similar comments')
    similar_parser.add_argument('case_id', help='Case ID')
    similar_parser.add_argument('evaluator', help='Evaluator username')
    similar_parser.add_argument('--top-k', type=int, default=5, help='Number of results')
    
    # Cluster command
    cluster_parser = subparsers.add_parser('cluster', help='Cluster comments')
    cluster_parser.add_argument('--n-clusters', type=int, default=5, help='Number of clusters')
    cluster_parser.add_argument('--method', choices=['kmeans', 'hierarchical'], 
                               default='kmeans', help='Clustering method')
    cluster_parser.add_argument('--show-comments', action='store_true',
                               help='Show full comments for each cluster')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    try:
        store = EmbeddingStore()
        
        if args.command == 'stats':
            stats = store.get_statistics()
            print("\n" + "=" * 80)
            print("EMBEDDING STATISTICS")
            print("=" * 80)
            print(f"Total embeddings: {stats['total_embeddings']}")
            print(f"Embedding dimension: {stats['embedding_dimension']}")
            print(f"Model: {stats['model']}")
            print(f"Generated at: {stats['generated_at']}")
            
            print("\nBy evaluator:")
            for evaluator, count in sorted(stats['evaluator_counts'].items()):
                print(f"  {evaluator}: {count}")
            
            print("\nBy decision:")
            for decision, count in sorted(stats['decision_counts'].items()):
                print(f"  {decision}: {count}")
            
            if stats['problem_axes_counts']:
                print("\nProblem axes:")
                for axis, count in sorted(stats['problem_axes_counts'].items(), 
                                        key=lambda x: x[1], reverse=True):
                    print(f"  {axis}: {count}")
            
            print("=" * 80)
        
        elif args.command == 'similar':
            print(f"\nüîç Finding comments similar to {args.case_id} by {args.evaluator}...")
            
            results = store.find_similar_to_comment(
                args.case_id,
                args.evaluator,
                args.top_k
            )
            
            if results:
                print(f"\nTop {len(results)} similar comments:\n")
                for i, item in enumerate(results, 1):
                    print(f"{i}. [{item['evaluator']}] {item['case_id'][:12]}... "
                          f"(similarity: {item['similarity']:.3f})")
                    print(f"   Decision: {item['decision']}")
                    if item['problem_axes']:
                        print(f"   Problem axes: {', '.join(item['problem_axes'])}")
                    print(f"   Comment: {item['comments']}")
                    print()
            else:
                print("No similar comments found.")
        
        elif args.command == 'cluster':
            print(f"\nüî¨ Clustering comments into {args.n_clusters} clusters...")
            print(f"   Method: {args.method}\n")
            
            result = store.cluster_comments(args.n_clusters, args.method)
            
            print(f"Cluster sizes:")
            for label in sorted(result['cluster_sizes'].keys()):
                size = result['cluster_sizes'][label]
                print(f"  Cluster {label}: {size} comments")
            
            if args.show_comments:
                print("\n" + "=" * 80)
                for label in sorted(result['clusters'].keys()):
                    print(f"\nCLUSTER {label} ({result['cluster_sizes'][label]} comments)")
                    print("-" * 80)
                    
                    for item in result['clusters'][label]:
                        print(f"‚Ä¢ [{item['evaluator']}] {item['case_id'][:12]}... "
                              f"({item['decision']})")
                        if item['problem_axes']:
                            print(f"  Axes: {', '.join(item['problem_axes'])}")
                        print(f"  \"{item['comments']}\"")
                        print()
    
    except FileNotFoundError as e:
        print(f"\n‚ùå {e}")
        return 1
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

